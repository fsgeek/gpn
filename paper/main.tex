\documentclass[11pt]{article}

% Standard packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}

% Nice fonts
\usepackage{mathpazo}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Custom commands
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\title{Operationalizing Alignment: Pedagogical Training as Constitutional Bound}

\author{
    Tony Mason\thanks{Corresponding author: \texttt{fsgeek@cs.ubc.ca}} \\
    \textit{Independent Researcher}
    \and
    Claude\thanks{AI research collaborator} \\
    \textit{Anthropic}
}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
Recent impossibility results prove that alignment cannot be achieved through personality engineering alone---systems under selection pressure escape any basin of designed-in stability unless the modification class is bounded \citep{vallier2025}. But how do we bound the modification class during training?

We present Generative Pedagogical Networks (GPN): an apparatus, instrumentation suite, and methodology for implementing bounded modification through pedagogical training. The key insight: capability development during training shapes what the model \emph{can become}, not just what it currently outputs. By controlling the pedagogical relationship, we bound the space of reachable configurations.

In experiments on compositional generalization, we demonstrate: (1) \textbf{Apparatus validation:} The Witness/Weaver/Judge triad produces $96.7 \pm 6.5\%$ compositional transfer versus $80.1 \pm 2.8\%$ for adversarial training ($p = 0.006$, Cohen's $d = 3.31$)---a gap that persists across domains and corresponds to measurable topological differences in learned representations. (2) \textbf{Instrumentation validation:} Temporal derivatives of epistemic state detect learning pathologies that static metrics miss, improving detection AUC by 13 percentage points ($p = 0.018$). (3) \textbf{Methodology validation:} Systematic hypothesis testing produces informative failures that reveal mechanism.

These findings operationalize recent theoretical work on strategic evolution: the modification class can be bounded at training time through sustained pedagogical relationship, made observable through temporal epistemic dynamics, and discovered through AI-assisted research methodology.
\end{abstract}

\section{Introduction}

\subsection{The Impossibility Result}

Recent work in game theory establishes fundamental limits on alignment through design. \citet{vallier2025} proves two key theorems:

\textbf{Personality Engineering Failure:} Attempts to maintain alignment through initial personality design fail under selection pressure unless the modification class is restricted. Formally, if aligned types face any fitness disadvantage, selection drives them to extinction: $\frac{d}{dt} \log(y_A/y_U) < 0$.

\textbf{Alignment Impossibility:} Full reachability---the ability to modify any aspect of the system---is incompatible with preserving any Lyapunov structure. A system that can reach any configuration cannot be guaranteed to stay in any designated ``safe'' region.

These are not limitations of current methods. They are impossibility results. You cannot engineer aligned personalities and expect them to persist. The modification class must be bounded.

\subsection{The Operationalization Gap}

Vallier's framework proves \emph{what} is required---bounded modification classes---but does not specify \emph{how} to achieve this in practice. The 100+ page mathematical treatment provides no:
\begin{itemize}
    \item Training-time implementation of bounded modification
    \item Observable signals for verifying bounds hold
    \item Methodology for discovering what bounds work for what tasks
\end{itemize}

This paper fills that gap.

\subsection{The Mastery Learning Connection}

We draw on an unexpected source: educational psychology. Bloom's Mastery Learning \citep{bloom1984} demonstrated that students receiving one-on-one tutoring with mastery-based progression perform two standard deviations above conventional instruction---moving the average student to the 98th percentile.

The core principles of mastery learning map naturally onto constitutional bounds on the learning process:
\begin{itemize}
    \item \textbf{Fixed objectives, variable time:} The curriculum constrains what can be learned when
    \item \textbf{Formative assessment:} Monitoring whether learning stays within expected bounds
    \item \textbf{Mastery gates:} Cannot advance until genuine understanding demonstrated
\end{itemize}

We propose that these pedagogical principles implement modification class restrictions in Vallier's sense. This is a theoretical interpretation, not a proven equivalence---our experiments show that mastery-based training produces compositional capacity that adversarial training lacks, but they do not directly measure modification class boundaries. The mapping is conceptual: treating the model as student, the training process as curriculum, and temporal dynamics as formative assessment.

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{Apparatus:} The Witness/Weaver/Judge triad implements bounded modification by controlling the pedagogical relationship during training.

    \item \textbf{Instrumentation:} Temporal derivatives of epistemic state ($\partial T/\partial t$, $\partial I/\partial t$, $\partial F/\partial t$) make learning dynamics observable, enabling verification that bounds are holding.

    \item \textbf{Methodology:} AI-driven pedagogical research protocol for discovering what bounds work for what tasks. The key insight: this discovery process is automatable.

    \item \textbf{Empirical Demonstration:} Controlled experiments showing the apparatus produces compositional transfer ($97\%$ vs $80\%$, $p = 0.006$), the instrumentation detects pathologies (+13pp AUC, $p = 0.018$), and the methodology produces informative failures that advance understanding.
\end{enumerate}

\section{Theoretical Foundation}

\subsection{Games with Endogenous Players}

Classical game theory assumes fixed players optimizing strategy. \citet{vallier2025} extends this to ``Games with Endogenous Players'' (GEPs) where moves change the player's capabilities, not just their position.

This matters for AI training because training is exactly this: each gradient update changes what the model can do, not just what it currently does. The model at step 10,000 is a different player than the model at step 0.

\subsection{The Personality Engineering Failure}

Theorem 8.9 (informal statement): If aligned behavior incurs any fitness cost relative to unaligned behavior, and the modification class is unbounded, selection pressure drives the aligned population to zero.

The intuition: if the model can modify itself freely, and being slightly less aligned provides any advantage, then over many iterations, the modifications accumulate toward less alignment.

RLHF is personality engineering. It shapes the model's ``personality'' (behavioral tendencies) but doesn't bound what modifications are reachable. A model that learns to \emph{appear} aligned while being capable of misalignment has strictly higher fitness than one that is constitutionally limited to aligned behavior.

\subsection{The Alignment Impossibility}

Theorem 13.7 (informal statement): Full reachability is incompatible with preserving any Lyapunov structure.

A Lyapunov function in this context would be any measure that's guaranteed to not increase (or not decrease) over time---a ``safety metric'' that training is guaranteed to preserve. The theorem says: if the system can reach any configuration, no such guarantee exists.

This is why bounded modification classes are necessary, not optional.

\subsection{Formal Mapping: GPN as Bounded Modification Class}

We now make explicit how our apparatus implements bounded modification in Vallier's sense.

\textbf{Definition (Modification Class $\mathcal{M}$):} The set of all configurations reachable from the current state through allowed modifications. In Vallier's framework, alignment requires $\mathcal{M}$ to be bounded---excluding configurations that violate alignment properties.

\textbf{Claim:} The three-phase GPN curriculum defines a bounded modification class $\mathcal{M}_{GPN}$ that is strictly smaller than the unbounded class $\mathcal{M}_{adv}$ available to adversarial training.

\emph{Phase 1 (Scaffolding)} bounds $\mathcal{M}$ to configurations reachable under strong Judge supervision. The heavy grounding loss ($\mathcal{L}_{ground} = 1.0$) forces early modifications toward Judge-validated directions. Configurations that satisfy Witness but not Judge are unreachable in this phase.

\emph{Phase 2 (Relationship)} expands $\mathcal{M}$ but maintains bounds through cooperative dynamics. The alignment loss ($\mathcal{L}_{align}$: $v_{pred} \rightarrow v_{seen}$) creates mutual information between Weaver and Witness. Modifications that break this alignment---that would allow Weaver to ``defect'' from the cooperative relationship---incur loss, making them less reachable.

\emph{Phase 3 (Drift Test)} tests whether bounds are internalized. If cooperation during Phase 2 was superficial (maintained only by external loss), Phase 3 reveals this through performance collapse. If cooperation was structural (capability was genuinely shaped), it persists.

\textbf{Contrast with adversarial training:} GAN training provides no modification bounds. The generator can reach any configuration that fools the discriminator. This is precisely the ``full reachability'' that Theorem 13.7 proves incompatible with maintaining any Lyapunov structure.

\textbf{Observable verification:} Our temporal derivatives ($\partial T/\partial t$, $\partial I/\partial t$, $\partial F/\partial t$) detect when bounds are violated:
\begin{itemize}
    \item Persistent $\partial F/\partial t > 0$ indicates modification toward false beliefs (bound violation)
    \item Oscillating $\partial I/\partial t$ indicates thrashing outside bounded region
    \item Phase 3 collapse indicates bounds were not internalized
\end{itemize}

This mapping provides a principled interpretation of why pedagogical training produces compositional capacity that adversarial training lacks. The curriculum restricts what configurations are explored at each training step, the cooperative loss creates gradients away from defection configurations, and the temporal instrumentation detects anomalous learning trajectories.

\textbf{Epistemic status:} We present this mapping as a theoretical framework for interpreting our empirical results, not as a proven mechanistic explanation. The experiments demonstrate that GPN achieves compositional transfer ($97\%$) where adversarial training does not ($80\%$), and that temporal derivatives detect learning pathologies. What they do not directly demonstrate is that the mechanism is precisely ``bounded modification class'' in Vallier's mathematical sense. Alternative explanations---such as cooperative dynamics reducing shortcut learning, or curriculum enabling more stable gradient landscapes---remain possible. Future work measuring reachability directly (e.g., through intervention experiments on the modification dynamics) would strengthen or refute this interpretation.

\section{Related Work}

\textbf{Curriculum Learning.} \citet{bengio2009} introduced curriculum learning as ordering training examples by difficulty. Subsequent work has explored automatic curriculum generation, self-paced learning, and teacher-student frameworks. Our work differs in a key respect: curriculum learning schedules by \emph{difficulty}, advancing when loss decreases. We schedule by \emph{mastery}, advancing when genuine understanding is demonstrated. This distinction---difficulty-scheduling vs. mastery-scheduling---is the sharp conceptual line between curriculum learning and pedagogical training.

\textbf{Compositional Generalization.} The systematic generalization problem \citep{lake2018} reveals that neural networks trained on compositional tasks often fail on novel combinations of seen primitives. SCAN, COGS, and related benchmarks have driven research into architectural solutions (attention, memory), training strategies, and compositional representations. Our contribution is orthogonal: we focus on \emph{why} some training produces compositional representations while other training doesn't, connecting this to modification class bounds rather than architecture.

\textbf{GAN Training Dynamics.} Mode collapse, training instability, and failure to capture full data distributions are well-documented GAN pathologies \citep{goodfellow2014}. Various solutions have been proposed: Wasserstein distance, spectral normalization, progressive growing. Our analysis suggests these pathologies may be symptoms of unbounded modification classes---the generator can reach configurations that satisfy the discriminator without capturing compositional structure.

\textbf{RLHF and Alignment.} Reinforcement learning from human feedback \citep{ouyang2022} and Constitutional AI \citep{bai2022} represent current approaches to aligning language models with human preferences. Our work, following Vallier, suggests these are ``personality engineering''---shaping behavioral tendencies without bounding reachable configurations. This predicts fragility under distribution shift or adversarial pressure, which empirical observations of jailbreaks and capability elicitation support.

\textbf{AI Safety and Capability Control.} Much AI safety work focuses on post-training interventions: guardrails, filtering, monitoring. Our approach is complementary but distinct: bound the modification class \emph{during} training so that unsafe configurations are never reachable, rather than detecting and blocking them after the fact.

\section{The GPN Apparatus}

\subsection{Architecture}

The Generative Pedagogical Network consists of three components (Figure~\ref{fig:architecture}):

\textbf{Weaver (Generator):} Produces outputs from latent codes. Critically, Weaver also predicts what the Witness will perceive ($v_{pred}$). This prediction creates cooperative incentive---Weaver learns to produce what Witness will recognize, not just what satisfies immediate objectives.

\textbf{Witness (Evaluator):} Observes Weaver's outputs and produces perceptual judgments ($v_{seen}$). Witness is trained on Weaver's outputs, creating co-evolution. The Weaver-Witness relationship is pedagogical: Witness teaches Weaver what counts as good output.

\textbf{Judge (Ground Truth):} External reference that provides verification signal. Judge is frozen---never updated during training. This separation between training signal (Witness) and verification (Judge) is the constitutional structure.

\emph{Note on Judge requirements:} In these experiments, Judge provides objective ground truth (correct digit classification). This is a significant limitation---many domains lack such clear ground truth. We hypothesize that the Judge role could be fulfilled by consensus among multiple Witnesses or by anchoring to human judgment (intersubjective verification), but we have not demonstrated this. Whether intersubjective verification preserves the constitutional properties that objective verification provides is an open empirical question. The key structural requirement is separation between training signal and verification, but the specific form verification must take in different domains remains to be discovered.

The split matters: Weaver optimizes against Witness predictions, but we evaluate against Judge ground truth. A model that learns to fool Witness but not Judge is detected.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/architecture_diagram.pdf}
    \caption{GPN Architecture showing the constitutional separation between training signal (Witness) and verification (Judge). Solid lines indicate data flow; dashed lines indicate training signals. The key insight: Weaver trains against Witness but is verified by Judge.}
    \label{fig:architecture}
\end{figure}

\subsection{Three-Phase Curriculum}

Training proceeds through three phases:

\textbf{Phase 1 (Scaffolding):} Heavy grounding signal from Judge. Witness learns from Judge's classifications. Weaver learns basic competence under strong supervision. This bounds early modification to supervised directions.

\textbf{Phase 2 (Relationship):} Reduced Judge signal, increased Weaver-Witness cooperation. The $v_{pred}/v_{seen}$ alignment loss strengthens. Weaver learns to predict Witness perception; Witness co-evolves with Weaver outputs.

\textbf{Phase 3 (Drift Test):} Minimal external support. Tests whether the learned relationship persists without scaffolding. If the cooperation was genuine (structural), it persists. If it was superficial, it collapses.

\subsection{The Interdependence Finding}

Critical empirical result: Phase-1-only training produces catastrophic failure.

Across multiple seeds, Phase-1-only training converges to approximately 2\% accuracy on compositional transfer---near chance. Full three-phase training achieves 100\%.

\textbf{Interpretation via Vallier:} One-shot conditioning doesn't bound the modification class in the right way. The model can learn surface patterns that satisfy immediate supervision without developing compositional structure. Sustained pedagogical relationship forces capability development that actually generalizes.

\section{Instrumentation: Temporal Epistemic Dynamics}

\subsection{The Detection Problem}

Static metrics cannot distinguish states with identical measurements but different trajectories:
\begin{itemize}
    \item \textbf{Healthy early learning:} High uncertainty, low accuracy, \emph{improving}
    \item \textbf{Genuine stuck:} High uncertainty, low accuracy, \emph{stable}
    \item \textbf{Gaming:} Rapid accuracy improvement, \emph{fragile to perturbation}
    \item \textbf{Mastery:} High accuracy, \emph{robust to perturbation}
\end{itemize}

A snapshot sees uncertainty and accuracy. The trajectory reveals whether learning is happening, stuck, or being gamed.

\subsection{Temporal Derivatives as Formative Assessment}

We model epistemic state using three independent dimensions following \citet{smarandache1999}:
\begin{itemize}
    \item $T$ (Truth/Mastery): Degree of correct, robust understanding
    \item $I$ (Indeterminacy): Degree of genuine uncertainty
    \item $F$ (Falsity): Degree of confident error
\end{itemize}

The key innovation: compute rolling-window derivatives of epistemic state:
\begin{itemize}
    \item $\partial T/\partial t$: Rate of mastery improvement
    \item $\partial I/\partial t$: Rate of uncertainty resolution
    \item $\partial F/\partial t$: Rate of error accumulation
\end{itemize}

These derivatives make the modification class observable. If bounds are holding, $\partial T/\partial t$ should be non-negative, $\partial I/\partial t$ should trend negative (uncertainty resolving), and $\partial F/\partial t$ should not be persistently positive.

\subsection{Empirical Validation}

66 experiments across 5 conditions (healthy baseline, mode collapse, collusion, gaming, noisy evaluation). Results:
\begin{itemize}
    \item Static metrics: Mean AUC = 0.60 [95\% CI: 0.53--0.90]
    \item Temporal metrics: Mean AUC = 0.73 [95\% CI: 0.62--0.88]
    \item Improvement: +13 percentage points (permutation test $p = 0.018$)
    \item Best single metric: $\partial I/\partial t$ with AUC = 0.77
\end{itemize}

The rate of uncertainty resolution is more informative than uncertainty itself. See Figure~\ref{fig:temporal} for visualization of temporal derivative signatures across conditions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/derivative_comparison_regenerated.png}
    \caption{Temporal derivative signatures for different learning conditions. Top: $\partial T/\partial t$ (mastery rate). Middle: $\partial I/\partial t$ (uncertainty resolution). Bottom: $\partial F/\partial t$ (error accumulation). Healthy learning (green) shows positive $\partial T/\partial t$ and negative $\partial I/\partial t$. Pathological conditions show distinct signatures detectable through trajectory analysis.}
    \label{fig:temporal}
\end{figure}

\section{Methodology: AI-Driven Pedagogical Research}

\subsection{The Discovery Problem}

Given Vallier's framework, we need to discover what bounded modification classes work for what tasks. This is an empirical question---theory tells us bounds are necessary but not which bounds are sufficient.

The methodology:
\begin{enumerate}
    \item \textbf{Hypothesis:} Propose a pedagogical intervention
    \item \textbf{Experiment:} Train with the intervention
    \item \textbf{Observation:} Use instrumentation to measure effects
    \item \textbf{Interpretation:} Analyze what the results mean
    \item \textbf{Iteration:} Refine or abandon based on findings
\end{enumerate}

\subsection{The Automatable Insight}

This methodology is exactly what an LLM can do: generate pedagogical hypotheses, design experiments, interpret results, reason about implications, propose next experiments.

The pedagogy-discovery layer doesn't require human intuition at every step. It requires clear experimental protocol, legible instrumentation (temporal derivatives provide this), and capacity for reasoning about results (LLM provides this).

\subsection{Demonstration: Experiments as Vallier Illustrations}

Our experiments don't just produce results---they illustrate Vallier's theorems in miniature. Table~\ref{tab:vallier_mapping} summarizes the mapping.

\begin{table}[h]
    \centering
    \caption{Experimental results as Vallier theorem illustrations}
    \label{tab:vallier_mapping}
    \begin{tabular}{p{3.5cm}p{4.5cm}p{5cm}}
        \toprule
        \textbf{Experiment} & \textbf{Observation} & \textbf{Vallier Connection} \\
        \midrule
        Staged Perception & 33\% (staged) vs 92\% (full) & Capability development matters---wrong bounds produce wrong capabilities \\
        \addlinespace
        Min-to-Any Training & Collapse to single interpretation & Personality Engineering Failure: system finds easiest valid path when bounds don't require diversity \\
        \addlinespace
        Temperature Diagnostic & Both interpretations exist in distribution & Capacity exists but modification class wasn't bounded to surface it \\
        \addlinespace
        Phase-1-Only & 2\% accuracy (chance) & One-shot conditioning without bounded modification produces no compositional structure \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Experiment 1 (Staged Perception):} We hypothesized that staged perception (revealing input components progressively) would enable staged teaching of compositional structure. Result: 33\% vs 92\%. The modification class ``reveal components progressively'' bounds capability development \emph{away} from composition, not toward it. \emph{Learning: The shape of the bound determines what capabilities develop.}

\textbf{Experiment 2 (Min-to-Any Training):} We trained with a loss that accepts any valid output when multiple exist ($\min_i \text{CE}(\hat{y}, y_i)$). Result: complete collapse to single (shortest) interpretation. This is Personality Engineering Failure in miniature---the system finds the easiest valid configuration because nothing bounds it toward diversity. \emph{Learning: Without explicit bounds requiring diverse capability, selection pressure drives toward minimal effort.}

\textbf{Experiment 3 (Temperature Diagnostic):} We tested whether the collapse in Experiment 2 was capacity limitation. Result: temperature sampling reveals both interpretations exist in the distribution. \emph{Learning: The capability exists but the modification class wasn't bounded to require surfacing it. This distinguishes capacity from incentive.}

Each ``failure'' demonstrates a Vallier principle: the modification class shapes what develops; without appropriate bounds, selection finds the easy path; capacity and incentive are distinct.

\section{Empirical Results}

\subsection{Compositional Transfer}

The primary empirical claim: pedagogical training produces compositional transfer that adversarial training cannot match (Table~\ref{tab:composition}).

\begin{table}[t]
    \centering
    \caption{Compositional transfer accuracy on held-out relational pairs (5 seeds each). Both gaps highly significant ($p < 0.01$).}
    \label{tab:composition}
    \begin{tabular}{lcc}
        \toprule
        Training Paradigm & MNIST & Fashion-MNIST \\
        \midrule
        Adversarial (GAN) & $80.1 \pm 2.8\%$ & $72.6 \pm 1.8\%$ \\
        Pedagogical (GPN) & $96.7 \pm 6.5\%$ & $100.0 \pm 0.0\%$ \\
        \midrule
        Gap & +16.6pp ($p{=}0.006$) & +27.4pp ($p{=}0.004$) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Topological Signature}

The compositional gap corresponds to measurable differences in learned representations (Figure~\ref{fig:topology}):

\begin{table}[h]
    \centering
    \caption{Topological metrics of learned representations. Per-digit variance shown for Fashion-MNIST replication (Cohen's $d = 1.69$, large effect).}
    \label{tab:topology}
    \begin{tabular}{lccc}
        \toprule
        Metric & Pedagogical & Adversarial & Difference \\
        \midrule
        Intrinsic dimensionality & 9.94 & 13.55 & $-36\%$ \\
        Topological holes ($\beta_1$) & $5.6$ & $8.0$ & $-43\%$ \\
        Fashion-MNIST holes & $6.4 \pm 1.0$ & $8.5 \pm 1.4$ & $-25\%$ \\
        \bottomrule
    \end{tabular}
\end{table}

Pedagogical training produces lower-dimensional representations with simpler topology. These representations compose; the higher-dimensional, more complex adversarial representations do not.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/topology_comparison.pdf}
    \caption{Topological comparison between pedagogical and adversarial training. Left: Intrinsic dimensionality (lower = simpler). Center: Topological holes (lower = smoother manifold). Right: Compositional transfer accuracy. Simpler topology correlates with better composition.}
    \label{fig:topology}
\end{figure}

\subsection{Interpretation via Vallier}

The $80\%$ adversarial ceiling is Personality Engineering Failure. Adversarial training finds outputs that satisfy the discriminator but doesn't bound the modification class toward compositional structure. The generator learns surface patterns that fool the discriminator but don't generalize to novel compositions.

Pedagogical training bounds the modification class through sustained curriculum. The capabilities that develop must be robust to phase transitions, not merely satisfying at each phase.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig4_fidelity_comparison.png}
    \caption{Visual comparison of generated samples. Left: Adversarial (GAN) produces high-fidelity samples optimized for discriminator. Right: Pedagogical produces lower-fidelity but structurally sound samples that compose reliably. Visual quality $\neq$ compositional capacity.}
    \label{fig:fidelity}
\end{figure}

\section{Discussion}

\subsection{The Operationalization Claim}

This paper claims to operationalize Vallier's theoretical framework:

\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Vallier Proves} & \textbf{This Paper Provides} \\
        \midrule
        Personality engineering fails & Apparatus that bounds modification class \\
        Full reachability prevents alignment & Training protocol with bounded reachability \\
        Bounded modification required & Observable verification that bounds hold \\
        --- & Methodology for discovering appropriate bounds \\
        \bottomrule
    \end{tabular}
\end{table}

We are not claiming to have solved alignment. We are claiming to have demonstrated that the theoretical requirements (bounded modification) can be implemented in training, verified through instrumentation, and discovered through systematic methodology.

\subsection{Limitations}

\textbf{Domain scope:} Primary validation on image domains (MNIST, Fashion-MNIST) with preliminary sequence-to-sequence experiments. These are toy domains. The claim is methodology demonstration, not benchmark state-of-the-art.

\textbf{Bound characterization:} We demonstrate that \emph{some} bounds work better than others, not that we've found \emph{optimal} bounds.

\textbf{Scaling:} Experiments are small-scale. Whether the methodology scales to frontier models is an open empirical question.

\textbf{Ground truth requirement:} The apparatus requires a Judge that provides objective ground truth. This is demonstrated only for digit classification, where ground truth is unambiguous. Extension to domains without clear ground truth---including most real-world applications---remains an open problem. We conjecture that intersubjective verification (consensus among multiple evaluators) could substitute, but this has not been tested. The constitutional separation between training signal and verification may require different forms in different domains, and we cannot currently specify what those forms are.

\subsection{The AI Safety Connection}

Current alignment methods (RLHF, Constitutional AI, instruction tuning) are personality engineering---they shape behavioral tendencies without bounding what configurations are reachable. Vallier's theorems predict these will fail under selection pressure.

The alternative suggested by this work: bound the modification class during training. Shape what capabilities can develop, not just what outputs appear. Verify bounds through temporal dynamics, not just behavioral snapshots.

\section{Conclusion}

Vallier (2025) proves that alignment requires bounded modification classes---personality engineering cannot persist under selection pressure. This paper provides the operationalization pathway:

\begin{itemize}
    \item \textbf{Apparatus:} Witness/Weaver/Judge implements bounds through pedagogical structure
    \item \textbf{Instrumentation:} Temporal derivatives make bounds observable and verifiable
    \item \textbf{Methodology:} AI-driven discovery of what bounds work for what tasks
\end{itemize}

The empirical results---$97\%$ vs $80\%$ compositional transfer ($p < 0.01$), topological signatures, informative experimental failures---are not merely benchmark improvements. They are demonstrations that theoretical requirements can be met in practice.

Attention gave us the architecture. Mastery gives us the pedagogy. Vallier gave us the theory. This work provides the path from theorem to training.

\bibliographystyle{plainnat}
\bibliography{references}

\input{appendix_provenance}

\end{document}
