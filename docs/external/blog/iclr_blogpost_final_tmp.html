<!DOCTYPE html>
<html>
<head>
<title>iclr_blogpost_final.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h2 id="the-question">The Question</h2>
<p>Can neural networks learn compositional rules that generalize beyond their training data? The answer depends critically on the training objective used to build their elementary concepts.</p>
<p>The compositional generalization literature has established that neural networks can succeed on novel combinations of known primitives. A model that learns &quot;red circle&quot; and &quot;blue square&quot; can often generate &quot;red square.&quot; But what counts as a &quot;known&quot; primitive?</p>
<p>Consider a generative model pre-trained to produce images of the digit 7. Does high visual fidelity—the ability to render a photorealistic 7—constitute &quot;knowing&quot; the digit well enough to use it compositionally in relational tasks like &quot;generate X &gt; Y&quot;?</p>
<p>Intuitively, we assume better primitives yield better composition. We assume that if a model can generate a crisp, perfect digit, it must understand that digit. <strong>We found the opposite.</strong> In a controlled experiment, we show that high-fidelity primitives trained adversarially (GANs) hit a &quot;glass ceiling&quot; of composability, while low-fidelity &quot;blotchy&quot; primitives trained pedagogically achieve perfect transfer.</p>
<h2 id="the-experiment">The Experiment</h2>
<p>To investigate this boundary, we designed a deliberately simple experiment using Relational MNIST. The task: generate three-digit displays of the form [X][&gt;][Y] where X and Y are MNIST-style digits and X &gt; Y numerically.</p>
<p>The simplicity is intentional. MNIST is the petri dish, not the ecology. If the coverage boundary fails to appear here—in the most controlled possible environment—it would suggest the phenomenon is an artifact of complexity. That it appears so sharply in this minimal setting implies a fundamental property of neural compositionality that scale may mask but cannot cure.</p>
<p>Our approach follows <strong>pedagogical training with frozen primitives</strong>. We pre-trained a &quot;single-digit weaver&quot; to generate individual digits [0-9], then froze it and trained only a compositional layer—the &quot;latent splitter&quot;—to route latent codes for generating relational displays.</p>
<p>Crucially, we compared two types of teachers for the primitive generator:</p>
<ol>
<li><strong>Adversarial (GAN):</strong> Optimized to fool a discriminator, producing sharp, high-fidelity digits rich in texture.</li>
<li><strong>Pedagogical (Ours):</strong> Optimized for structural reconstruction, producing abstract, low-frequency representations.</li>
</ol>
<p><img src="file:///home/tony/projects/gpn/docs/external/blog/assets/img/fig4_fidelity_comparison.png" alt="Figure 1: The Fidelity Trap. Left: Standard GAN samples—high contrast and sharp edges, but carrying 'pseudo-texture' learned to satisfy an adversarial discriminator. Right: Our pedagogical samples—visually 'blotchy' and diffuse, prioritizing topological clarity over textural noise."></p>
<h2 id="the-fidelity-trap">The Fidelity Trap</h2>
<p>A surprising observation emerged during training. The pedagogical teacher generated primitives that were visually &quot;blotchy&quot;—diffuse, soft-edged, and topologically abstract.</p>
<p>This apparent degradation was actually a feature. By removing texture (noise), the teacher forced the student to learn geometry (signal). The student couldn't &quot;game&quot; the metric by matching pixels; it had to learn the invariant structure of each digit.</p>
<p>This matters methodologically: because our primitives encode topology rather than texture, logical failures cannot be attributed to pixel-level distribution shift. The model knew the abstract form of &quot;7&quot; perfectly.</p>
<p><img src="file:///home/tony/projects/gpn/docs/external/blog/assets/img/fig3_experimental_design.png" alt="Figure 2: Experimental architecture. We freeze the primitive generator (whether GAN or Pedagogical) and train only the relational routing layer."></p>
<h2 id="the-finding-the-coverage-boundary">The Finding: The Coverage Boundary</h2>
<p>We first tested whether primitives could compose <em>without</em> specific relational training coverage.</p>
<ul>
<li><strong>Condition:</strong> Train on relations for digits [0-4]. Test on relations for [5-9].</li>
<li><strong>Result:</strong> 0% Transfer.</li>
</ul>
<p>The model produced recognizable digits in isolation but garbage in relational contexts. This establishes the <strong>Coverage Boundary</strong>: Primitive Competence (drawing a 7) does not grant Compositional License (using a 7 in a relation). License is only acquired when a primitive appears in a relational context during training.</p>
<h2 id="the-showdown-the-glass-ceiling">The Showdown: The Glass Ceiling</h2>
<p>Once we established that coverage is necessary, we asked the deeper question: <strong>Is coverage sufficient?</strong></p>
<p>If we give an adversarial model every advantage—full coverage, identical architecture, and visually superior primitives—can it match pedagogical performance?</p>
<p>We ran the experiment on &quot;Novel Combinations.&quot; We trained on nearly all valid pairs (41 of 45), holding out 4 specific combinations (e.g., &quot;7&gt;3&quot;). Both models had seen every digit in diverse relational contexts.</p>
<p><strong>The Result:</strong></p>
<table>
<thead>
<tr>
<th>Training Objective</th>
<th>Primitives</th>
<th>Held-out Relation Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pedagogical (Ours)</td>
<td>Blotchy</td>
<td><strong>100.0%</strong></td>
</tr>
<tr>
<td>Adversarial (GAN)</td>
<td>Crisp</td>
<td><strong>81.1%</strong></td>
</tr>
</tbody>
</table>
<p>The adversarial model wasn't broken. 81% is not failure—it's a ceiling. The model had full relational coverage. It had seen every digit in compositional context. Yet it could not fully compose.</p>
<p>This is the <strong>Glass Ceiling of Adversarial Training</strong>. The model pays a <strong>tax on composition</strong>—capacity spent maintaining the illusion of texture leaves representations entangled in ways that resist perfect reassembly. No amount of additional coverage can break through, because the limitation is geometric, not statistical.</p>
<p><img src="file:///home/tony/projects/gpn/docs/external/blog/assets/img/fig1_coverage_boundary.png" alt="Figure 3: The coverage boundary and glass ceiling. Same architecture, different training objectives, opposite outcomes."></p>
<h2 id="why-it-matters-contract-of-appearance-vs-contract-of-meaning">Why It Matters: Contract of Appearance vs. Contract of Meaning</h2>
<p>This experiment is a critique of how we train generative models.</p>
<p>We currently train under a <strong>Contract of Appearance</strong>. Adversarial objectives (like GANs) and preference optimization (like RLHF) teach models to <em>mimic the surface statistics</em> of a correct answer. As our GAN results show, this produces high-fidelity primitives that look perfect to a critic but are hollow to a composer. They possess <strong>Primitive Competence</strong> but lack <strong>Compositional License</strong>.</p>
<p>Our pedagogical approach enforces a <strong>Contract of Meaning</strong>. By using &quot;blotchy,&quot; abstract primitives, we denied the model the ability to mimic. We forced it to learn the topology of the digit—the &quot;Platonic form&quot;—because the texture was unavailable.</p>
<p>We hypothesize that safety and generalization are the same goal. A model that truly understands the structure of a concept (rather than just its likelihood) may be a model that can be trusted to handle it in novel contexts. If this holds, fixing the fragility of AI may require us to stop optimizing for how things <em>look</em> and start optimizing for how they <em>compose</em>.</p>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>What's Novel</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Phase 1.5</strong></td>
<td>Novel Primitives (No Coverage)</td>
<td><strong>0% Transfer</strong> (The Coverage Boundary)</td>
</tr>
<tr>
<td><strong>Phase 1.6</strong></td>
<td>Adversarial Primitives (Full Coverage)</td>
<td><strong>81.1% Accuracy</strong> (The Glass Ceiling)</td>
</tr>
<tr>
<td><strong>Phase 1.6</strong></td>
<td>Pedagogical Primitives (Full Coverage)</td>
<td><strong>100% Accuracy</strong> (The Contract of Meaning)</td>
</tr>
</tbody>
</table>
<p>The coverage boundary tells us <em>when</em> composition is possible. The glass ceiling tells us <em>if</em> the primitives are capable of it. You need both: primitives shaped for meaning, and coverage that licenses their use.</p>
<p>Code and experimental details available at: [repository link upon acceptance]</p>

</body>
</html>
